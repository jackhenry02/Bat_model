{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeffress Coincidence Bank for Continuous Range Estimation (snntorch)\n",
    "\n",
    "Refactored notebook: from 3-class classification to a place-coded Jeffress bank that maps pulse\u2013echo delay (0\u2013100 ms) onto a spatial output layer. Output neuron index encodes estimated range; multiple spikes yield a continuous estimate via averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate, spikeplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Synthetic Data (Exact Delay Labels)\n",
    "- Input: 2 channels (pulse at t=0, echo at t=delay)\n",
    "- Delay: integer in [0, 100]\n",
    "- Label: the delay value itself (place code target index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 120  # covers up to 100 ms delay with margin\n",
    "max_delay = 100\n",
    "\n",
    "def generate_bat_data(num_samples: int):\n",
    "    data = torch.zeros(num_samples, time_steps, 2)\n",
    "    labels = torch.zeros(num_samples, dtype=torch.long)\n",
    "    for i in range(num_samples):\n",
    "        delay = np.random.randint(0, max_delay + 1)\n",
    "        jitter = np.random.choice([-1, 0, 1])\n",
    "        delay = int(np.clip(delay + jitter, 0, max_delay))\n",
    "        data[i, 0, 0] = 1.0          # pulse at t=0\n",
    "        data[i, delay, 1] = 1.0       # echo at t=delay\n",
    "        labels[i] = delay\n",
    "    return data, labels\n",
    "\n",
    "num_samples = 3000\n",
    "data, labels = generate_bat_data(num_samples)\n",
    "split = int(0.8 * num_samples)\n",
    "train_data, test_data = data[:split], data[split:]\n",
    "train_labels, test_labels = labels[:split], labels[split:]\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_data, train_labels), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_data, test_labels), batch_size=128, shuffle=False)\n",
    "print(f\"Train samples: {len(train_data)}, Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Jeffress SNN Architecture\n",
    "- Hidden: dense 200 LIF units (surrogate gradients)\n",
    "- Output: 101 neurons (0..100) as place code\n",
    "- Loss: CrossEntropy on spike counts per neuron (rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.97\n",
    "hidden_size = 200\n",
    "out_neurons = max_delay + 1  # 0..100 inclusive\n",
    "\n",
    "class JeffressSNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Surrogate gradient lets gradients flow through spike discontinuity\n",
    "        grad_fn = surrogate.fast_sigmoid()\n",
    "        self.fc1 = nn.Linear(2, hidden_size)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=grad_fn)\n",
    "        self.fc2 = nn.Linear(hidden_size, out_neurons)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=grad_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, time, 2]\n",
    "        batch_size = x.size(0)\n",
    "        mem1 = torch.zeros(batch_size, hidden_size, device=x.device)\n",
    "        mem2 = torch.zeros(batch_size, out_neurons, device=x.device)\n",
    "        spk2_rec = []\n",
    "        for t in range(x.size(1)):\n",
    "            cur = x[:, t, :]\n",
    "            h1 = self.fc1(cur)\n",
    "            spk1, mem1 = self.lif1(h1, mem1)\n",
    "            h2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(h2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "        return torch.stack(spk2_rec, dim=1)  # [batch, time, out_neurons]\n",
    "\n",
    "model = JeffressSNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "num_epochs = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training Loop\n",
    "- Spike counts over time become logits for CrossEntropyLoss.\n",
    "- Track loss and test MAE over epochs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from snntorch import utils\n",
    "\n",
    "# Debug: inspect a single batch\n",
    "x_dbg, y_dbg = next(iter(train_loader))\n",
    "print('Batch shape:', x_dbg.shape, 'Labels shape:', y_dbg.shape)\n",
    "# Expect [batch, time, features]; if not, permute helper below\n",
    "if x_dbg.dim() == 3:\n",
    "    print('Time dimension position (assuming batch-first): time axis=1, features axis=2')\n",
    "else:\n",
    "    print('Unexpected input rank:', x_dbg.dim())\n",
    "\n",
    "def to_time_first(x):\n",
    "    # convert [batch, time, features] -> [time, batch, features]\n",
    "    if x.dim() == 3:\n",
    "        return x.permute(1,0,2)\n",
    "    raise ValueError('Input must be 3D [batch,time,feat]')\n",
    "\n",
    "x_time_first = to_time_first(x_dbg)\n",
    "print('After permute to time-first:', x_time_first.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_spikes(spikes: torch.Tensor):\n",
    "    \"\"\"spikes: [batch, time, out_neurons] -> continuous distance estimate per sample.\"\"\"\n",
    "    counts = spikes.sum(dim=1)  # [batch, out_neurons]\n",
    "    total = counts.sum(dim=1, keepdim=True) + 1e-6\n",
    "    weighted = (counts * torch.arange(out_neurons, device=counts.device)).sum(dim=1)\n",
    "    avg_idx = weighted / total.squeeze(1)\n",
    "    # If no spikes, fallback to argmax of counts (which will be zero; argmax returns 0)\n",
    "    return avg_idx\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    maes = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            spk_out = model(x)\n",
    "            preds = decode_spikes(spk_out)\n",
    "            mae = (preds - y.float()).abs().mean().item()\n",
    "            maes.append(mae)\n",
    "    return float(np.mean(maes))\n",
    "\n",
    "train_losses = []\n",
    "test_maes = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        spk_out = model(x)\n",
    "        # Rate code as logits for CE\n",
    "        logits = spk_out.sum(dim=1)  # [batch, out_neurons]\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * x.size(0)\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "    mae = evaluate(model, test_loader)\n",
    "    test_maes.append(mae)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f} | Test MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluation & Visualization\n",
    "- Scatter: true vs. predicted delay\n",
    "- Error histogram\n",
    "- Jeffress raster: sweep delays 0..100, plot output-layer spikes across the population."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from snntorch import utils\n",
    "\n",
    "def train_epoch(net, loader, optimizer, criterion):\n",
    "    net.train()\n",
    "    total_loss = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Ensure time-first shape [time, batch, features]\n",
    "        x_tbf = x.permute(1,0,2)\n",
    "        utils.reset(net)  # reset state per batch\n",
    "        spk_rec = []\n",
    "        for t in range(x_tbf.size(0)):\n",
    "            spk = net(x_tbf[t])\n",
    "            spk_rec.append(spk)\n",
    "        spk_rec = torch.stack(spk_rec, dim=0)  # [time, batch, out_neurons]\n",
    "        logits = spk_rec.sum(dim=0)  # [batch, out_neurons]\n",
    "        loss = criterion(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def eval_mae(net, loader):\n",
    "    net.eval()\n",
    "    maes = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x_tbf = x.permute(1,0,2)\n",
    "            utils.reset(net)\n",
    "            spk_rec = []\n",
    "            for t in range(x_tbf.size(0)):\n",
    "                spk = net(x_tbf[t])\n",
    "                spk_rec.append(spk)\n",
    "            spk_rec = torch.stack(spk_rec, dim=0)\n",
    "            preds = decode_spikes(spk_rec.permute(1,0,2))  # back to [batch,time,out] for decode\n",
    "            maes.append((preds - y.float()).abs().mean().item())\n",
    "    return float(np.mean(maes))\n",
    "\n",
    "train_losses = []\n",
    "test_maes = []\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    train_losses.append(tr_loss)\n",
    "    mae = eval_mae(model, test_loader)\n",
    "    test_maes.append(mae)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | Loss: {tr_loss:.4f} | Test MAE: {mae:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}